# HSMA Session 4D

## Slides

<a href="https://docs.google.com/presentation/d/1LgVvgLzm4dxCdDCusjWWC_SBrtXhhCPchHXK4oI0kWU/edit?usp=sharing"><img src="https://img.shields.io/static/v1?label=Google+Slides&message=Click+here+to+view+the+slides+for+this+session&color=%23FBBC04&style=for-the-badge&logo=googleslides&logoColor=%23FBBC04" alt="Google Slides - Click here to view slides for this session"></a>

## Decision Trees & Random Forests

### Learning Objectives

Students should be able to:

- Explain the main points of how a decision tree works
- List the methods that may be used to determine splits (Gini Impurity, entropy, information gain)
- Explain why feature scaling is not required in decision trees
- Explain some benefits and downsides of decision trees
- Write code to classify a dataset using a decision tree using the sklearn library
- Write code to plot the resulting decision tree
- Explain some of the hyperparameters that may be set when using decision trees
    - Pruning
    - Minimum samples (leaf and split)
    - Maximum depth
- Write code to create a confusion matrix using the sklearn library
- Explain the benefits of normalising a confusion matrix
- Write code to create a normalised confusion matrix using the sklearn library
- Explain the main points of how random forests work
- List the two ways in which randomness is introduced into the tree building process
- Explain the concept of bootstrapping
- Explain the difference between sampling with and without replacement
- Explain some benefits and downsides of random forests
- Write code to classify a dataset using a random forest using the sklearn library
